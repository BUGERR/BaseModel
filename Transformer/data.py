import config
import os
import datasets
from tokenizers import Tokenizer
from tokenizers.models import BPE
from tokenizers.pre_tokenizers import Whitespace
from tokenizers.trainers import BpeTrainer
from tokenizers.processors import TemplateProcessing
from typing import Optional, Sequence
import torch
from torch.utils.data import DataLoader

SOS_TOKEN = "[SOS]"
EOS_TOKEN = "[EOS]"
PAD_TOKEN = "[PAD]"
UNK_TOKEN = "[UNK]"

special_tokens = [SOS_TOKEN, EOS_TOKEN, PAD_TOKEN, UNK_TOKEN]

def build_tokenizer(dataset, force_reload):
    '''
    pipeline: check cache --> ç©ºæ ¼é¢„åˆ†è¯ --> åˆå§‹åŒ– BpeTrainer --> å®šä¹‰ batch_iterator
               --> train_from_iterator --> enable padding å’Œ trancation --> åå¤„ç†
    '''
    tokenizer_path = config.dataset_dir / f"tokenizer.json"
    if os.path.exists(tokenizer_path) and not force_reload:
        tokenizer = Tokenizer(BPE(unk_token=UNK_TOKEN)).from_file(str(tokenizer_path))
    else:
        tokenizer = Tokenizer(BPE(unk_token=UNK_TOKEN))
        tokenizer.pre_tokenizer = Whitespace()
        trainer = BpeTrainer(
            # vocab_size = 37000, 
            min_frequency = 2,
            show_progress = True,
            special_tokens = special_tokens
        )
        print('Training tokenizer ...')

        def batch_iterator(batch_size, dataset):
            train_data = dataset["train"]
            scaled_batch_size = batch_size // 2
            for i in range(0, train_data.num_rows, scaled_batch_size):
                batch = train_data[i : i + scaled_batch_size]
                yield batch['en'] + batch['de']
        
        tokenizer.train_from_iterator(batch_iterator(config.batch_size, dataset), trainer)
        tokenizer.enable_padding(
            pad_id=tokenizer.token_to_id(PAD_TOKEN), 
            pad_token=PAD_TOKEN
        )
        tokenizer.enable_truncation(max_length=config.max_len)
        tokenizer.post_processor = TemplateProcessing(
            # å•å¥çš„æ¨¡æ¿
            single = f"{SOS_TOKEN} $A {EOS_TOKEN}",
            # è¾“å…¥æ˜¯æˆå¯¹å¥å­çš„æ¨¡ç‰ˆï¼Œä¸€èˆ¬ç”¨ä¸åˆ°
            # pair=,
            # æŒ‡å®šç‰¹æ®Š token å’Œè¯åº“å¯¹åº” id
            special_tokens=[
                (SOS_TOKEN, tokenizer.token_to_id(SOS_TOKEN)),
                (EOS_TOKEN, tokenizer.token_to_id(EOS_TOKEN))
            ]
        )

        tokenizer.save(str(tokenizer_path))
    return tokenizer


def load_data(
    src_lang, tgt_lang, splits: Optional[Sequence[str]] = None, force_reload=False
):
    if sorted((src_lang, tgt_lang)) != ["de", "en"]:
        raise ValueError("Available language options are ('de','en') and ('en', 'de')")
    
    all_splits = ["train", "validation", "test"]
    if splits is None:
        splits = all_splits
    elif not set(splits).issubset(all_splits):
        raise ValueError(f"Splits should only contain some of {all_splits}")

    dataset = datasets.load_dataset("bentrevett/multi30k")

    tokenizer = build_tokenizer(dataset, force_reload)

    def collate_fn(batch):
        src_batch, tgt_batch = [], []
        for item in batch:
            src_batch.append(item[src_lang])
            tgt_batch.append(item[tgt_lang])

        src_batch = tokenizer.encode_batch(src_batch)
        tgt_batch = tokenizer.encode_batch(tgt_batch)

        src_tensor = torch.LongTensor([item.ids for item in src_batch])
        tgt_tensor = torch.LongTensor([item.ids for item in tgt_batch])

        return src_tensor, tgt_tensor
    
    dataloaders = [
        DataLoader(
            dataset=dataset[split],
            batch_size=config.batch_size,
            collate_fn=collate_fn,
            shuffle=split == 'train'
        )
        for split in splits
    ]

    return (tokenizer, *dataloaders)


if __name__ == '__main__':
    src_lang = 'en'
    tgt_lang = 'de'

    tokenizer, train_dataloader, valid_dataloader = load_data(
        src_lang, tgt_lang, ['train', 'validation']
        )

    for batch in train_dataloader:
        src, tgt = batch
        print(src.shape)
        print(tgt.shape)
        print(src[0])
        print(tgt[0])
        break

    output = tokenizer.encode("Hello, y'all!", "How are you ğŸ˜ ?")
    print(output.tokens)
    output = tokenizer.encode("Welcome to the ğŸ¤— Tokenizers library.")
    print(output.tokens)
#     dataset = datasets.load_dataset("bentrevett/multi30k")
#     # print(dataset)
#     train_data, valid_data, test_data = (
#         dataset["train"],
#         dataset["validation"],
#         dataset["test"],
#     )
#     print(train_data[0])

#     batch = train_data[0: 4]
#     batch = batch['en'] + batch['de']
#     print(batch)
#     for item in batch:
#         print(item)


    # print(train_data['en'][0])
    # print(train_data[0]['en'])
    # print(train_data['en'][0: 2])
    # print(train_data[0 : 2]['en'])
    # data = train_data[0]['en'] + ' ' + train_data[0]['de']
    # print(data)

    # batch = train_data['en'][0: 4]
    # batch = train_data[0: 4]

    # data = batch['en'] + batch['de']
    # print(data)
    # print(type(data))
    # print([item for item in data])
    # print(batch)
    # for item in zip(batch['en'], batch['de']):
    #     print(item)
    # print([(item['en'] + ' ' + item['de']) for item in batch])
    
    # def batch_iterator(batch_size):
    #     scaled_batch_size = batch_size // 2
    #     for i in range(0, train_data.num_rows, scaled_batch_size):
    #         batch = train_data[i : i + scaled_batch_size]
    #         yield batch['en'] + batch['de']